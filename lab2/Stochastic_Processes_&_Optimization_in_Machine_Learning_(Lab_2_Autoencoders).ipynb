{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise on Autoencoders**\n",
        "\n",
        "<p>Within the scope of this exercise, you will examine how an autoencoder works for the classification of a dataset.</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "FJxOYmQGsJ9c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUfLRNadlqMU"
      },
      "outputs": [],
      "source": [
        "# train autoencoder for classification with no compression in the bottleneck layer\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the scikit-learn <i>make_classification()</i> function to define a synthetic classification dataset (2 classes) with 100 input features (columns) and 1,000 samples (rows).\n",
        "\n",
        "Attention: In the given problem, most of the features are 90% redundant, allowing the autoencoder later to learn a truly useful compressed representation."
      ],
      "metadata": {
        "id": "DxTYHY6MqXdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# synthetic classification dataset\n",
        "from sklearn.datasets import make_classification\n",
        "# define dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\n",
        "# number of input columns\n",
        "n_inputs = X.shape[1]\n",
        "# summarize the dataset\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvZStKrylrW6",
        "outputId": "3a0afd8f-5c2a-462e-c4c7-24d7884c50d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 100) (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will develop an autoencoder model [Multilayer Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron).\n",
        "\n",
        "The model will take all samples as input and then output the same values. Therefore, it will learn to reconstruct the same pattern.\n",
        "\n",
        "The autoencoder consists of two parts: the encoder and the decoder.\n",
        "\n",
        "Once the autoencoder is trained, the decoder will not be used again, and we keep only the encoder to compress input samples into the vectors produced by the bottleneck.\n",
        "\n",
        "In this first autoencoder, we will not compress the input at all, and we will use intermediate layers with the same number of connections as the number of input nodes (thus equal to the number of dimensions of the sample). This will be done so that the model learns almost perfectly and we can confirm that the model has been implemented correctly.\n",
        "\n",
        "Before defining and fitting the model, we will split the data into training and test sets and scale the input data by normalizing the values to the range 0–1.\n"
      ],
      "metadata": {
        "id": "Do8y5OsirqST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into train test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
        "# scale data\n",
        "t = MinMaxScaler()\n",
        "t.fit(X_train)\n",
        "X_train = t.transform(X_train)\n",
        "X_test = t.transform(X_test)"
      ],
      "metadata": {
        "id": "4q80eRAOlrg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define the encoder to have two hidden layers: the first with twice the number of input dimensions (e.g., 200) and the second with the original number of dimensions (100).\n"
      ],
      "metadata": {
        "id": "bESsv4FTuDRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define encoder\n",
        "visible = Input(shape=(n_inputs,))\n",
        "# encoder level 1\n",
        "e = Dense(n_inputs*2)(visible)\n",
        "e = BatchNormalization()(e)\n",
        "e = LeakyReLU()(e)\n",
        "# encoder level 2\n",
        "e = Dense(n_inputs)(e)\n",
        "e = BatchNormalization()(e)\n",
        "e = LeakyReLU()(e)\n",
        "# bottleneck\n",
        "n_bottleneck = n_inputs\n",
        "bottleneck = Dense(n_bottleneck)(e)"
      ],
      "metadata": {
        "id": "bfg4IAu1lro8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the decoder. The decoder will have the reverse structure of the encoder."
      ],
      "metadata": {
        "id": "OYjQdojUuzZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define decoder, level 1\n",
        "d = Dense(n_inputs)(bottleneck)\n",
        "d = BatchNormalization()(d)\n",
        "d = LeakyReLU()(d)\n",
        "# decoder level 2\n",
        "d = Dense(n_inputs*2)(d)\n",
        "d = BatchNormalization()(d)\n",
        "d = LeakyReLU()(d)\n",
        "# output layer\n",
        "output = Dense(n_inputs, activation='linear')(d)\n",
        "# define autoencoder model\n",
        "model = Model(inputs=visible, outputs=output)"
      ],
      "metadata": {
        "id": "C0uZPgH4lrsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we compile our model."
      ],
      "metadata": {
        "id": "yYOGOrbava3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile autoencoder model\n",
        "model.compile(optimizer='adam', loss='mse')"
      ],
      "metadata": {
        "id": "wAZytGAKlrvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can train the model to reproduce the input data and monitor the model’s performance compared to the test set.\n",
        "\n",
        "Question 1: What is the difference between mini-batches and epochs?\n"
      ],
      "metadata": {
        "id": "WwYSO4-wvqAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the autoencoder model to reconstruct input\n",
        "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))"
      ],
      "metadata": {
        "id": "bdAfIRN5lr0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the learning curves for the train and test dataset."
      ],
      "metadata": {
        "id": "dOCUfjzFwbsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend(title = \"datasets\")\n",
        "pyplot.title(\"Loss\")\n",
        "pyplot.show()"
      ],
      "metadata": {
        "id": "UvD6Bp3ulr3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Try running the model with a bottleneck of (a) 50 and (b) 25. What do you observe regarding the model’s results?\n",
        "\n",
        "Question 3: Try using three layers (the two existing ones and one additional layer with a bottleneck of 50). What differences do you observe?"
      ],
      "metadata": {
        "id": "cM6PAjvzym_j"
      }
    }
  ]
}